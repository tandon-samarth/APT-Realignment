{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from shapely.geometry import box\n",
    "from pyproj import Geod\n",
    "from matplotlib import pyplot as plt\n",
    "import sys \n",
    "sys.path.append('/mnt/c/Users/tandon/OneDrive - TomTom/Desktop/tomtom/Workspace/01_Rooftop_accuracy/APT-Realignment/src/apt_realignment')\n",
    "from utils.geometric_utils import read_vector_data, create_logger, get_nearest_poly , prep_polygons_asarr,download_osm_building_footprint\n",
    "from utils.haversine_distance import get_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geod = Geod(ellps=\"WGS84\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from os import stat\n",
    "from os.path import isfile\n",
    "\n",
    "data_path  = \"/mnt/c/Users/tandon/OneDrive - TomTom/Desktop/tomtom/Workspace/01_Rooftop_accuracy/BFP_Analysis_USA/data/data\"\n",
    "state = \"CT\"\n",
    "city = \"hartford\"\n",
    "parcel_dir = \"Parcels_09003\"\n",
    "\n",
    "apt_data_path = os.path.join(data_path,state,\"APT_2022_09_000_nam_usa_utx.shp\" )\n",
    "parcel_path = os.path.join(data_path,state,city,\"{}/{}.shp\".format(parcel_dir,parcel_dir))\n",
    "msft_building = os.path.join(data_path,state,'Connecticut.geojson')\n",
    "osm_building = os.path.join(data_path,state,city,'building_footprint.shp')\n",
    "\n",
    "print(\"APT data path :\",apt_data_path)\n",
    "print(\"Parcel Data:\",parcel_path)\n",
    "print(\"Building_geojson: \",msft_building)\n",
    "print(\"OSM building footprint: \",osm_building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bfp_parcel_overlap(land_parcels,building_footprints):\n",
    "    \n",
    "    print(\"Reading Land Parcel data .. \")\n",
    "    land_parcel_df = read_vector_data(land_parcels)\n",
    "    print(\"Reading Building Footprints ..  \")\n",
    "    footprint_df = read_vector_data(building_footprints)\n",
    "    \n",
    "    print(\"creating sjoin of land Parcel data and Building Footprints.. \")\n",
    "    building_within_parcel_df = gpd.sjoin(land_parcel_df, footprint_df, op='intersects', how='left')\n",
    "    building_within_parcel_df = building_within_parcel_df.dropna()  # drop columns with no Buildings\n",
    "\n",
    "    def __get_buildingfootprint(val):\n",
    "        return footprint_df['geometry'].loc[val]\n",
    "\n",
    "    def __get_building_roi(data: gpd.GeoSeries):\n",
    "        building_polygon = data['building_geometry']\n",
    "        parcel_polygon = data['geometry']\n",
    "        building_roi = None\n",
    "        try:\n",
    "            if building_polygon == np.nan:\n",
    "                building_roi = parcel_polygon\n",
    "            if building_polygon.area > parcel_polygon.area:\n",
    "                building_roi = parcel_polygon.intersection(building_polygon)\n",
    "            else:\n",
    "                building_roi = building_polygon\n",
    "        except:\n",
    "            logging.error(\"error for {},{}\".format(building_polygon, parcel_polygon))\n",
    "        return building_roi\n",
    "\n",
    "    building_within_parcel_df['building_geometry'] = building_within_parcel_df['index_right'].apply(lambda x: __get_buildingfootprint(x))\n",
    "\n",
    "    building_within_parcel_df['building_roi'] = building_within_parcel_df.apply(lambda x: __get_building_roi(x), axis=1)\n",
    "    building_within_parcel_df = building_within_parcel_df.drop(['index_right', 'release'],axis=1)\n",
    "    building_within_parcel_df = building_within_parcel_df.dropna()\n",
    "    return building_within_parcel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge OSM and MSFT_building footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(osm_building):\n",
    "    download_osm_building_footprint(county=city,state=state,out_path=osm_building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_osm_msft_data(msft_geojson,osm_geojson):\n",
    "    osm_gdf = read_vector_data(osm_geojson)\n",
    "    osm_bounds = box(*osm_gdf.total_bounds)\n",
    "\n",
    "    msft_df = read_vector_data(msft_geojson)\n",
    "    msft_df = msft_df.reset_index()\n",
    "\n",
    "    osm_in_msft_df = gpd.sjoin(osm_gdf,msft_df, op='intersects', how='left')\n",
    "    merged_data = osm_in_msft_df['geometry']\n",
    "\n",
    "    def check_within_bounds(x):\n",
    "        geo_polygon = None\n",
    "        if osm_bounds.contains(x):\n",
    "            geo_polygon = x\n",
    "        return geo_polygon\n",
    "\n",
    "    msft_geometries = msft_df.loc[~msft_df['index'].isin(osm_in_msft_df['index_right'].values)]['geometry']\n",
    "    final_geometries = msft_geometries.apply(lambda x: check_within_bounds(x))\n",
    "    final_gdf = merged_data.append(final_geometries[final_geometries.notna()])\n",
    "    return final_gdf\n",
    "\n",
    "merged_data = merge_osm_msft_data(msft_building,osm_building)\n",
    "merged_data.to_file(os.path.join(data_path,state,city,'msft_osm_building_footprint.shp'), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32389,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    POLYGON ((-72.68249 41.76437, -72.68242 41.764...\n",
       "1    POLYGON ((-72.71554 41.79882, -72.71535 41.798...\n",
       "2    POLYGON ((-72.71387 41.79993, -72.71392 41.799...\n",
       "3    POLYGON ((-72.67338 41.76273, -72.67296 41.762...\n",
       "4    POLYGON ((-72.67454 41.76341, -72.67445 41.763...\n",
       "Name: geometry, dtype: geometry"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data = merge_osm_msft_data(msft_building,county='bexar',state= 'Texas')\n",
    "print(merged_data.shape)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Land Parcels and Building footprints\n",
    "\n",
    "This code read the parcel and building footprint data and finds out sjoin of both geometries . It rejects the data where building footprints is not found ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_bfp_df = get_bfp_parcel_overlap(land_parcels=parcel_path,building_footprints=msft_building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_bfp_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get BFP within Parcel data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_within_parcel_count = parcel_bfp_df.groupby('PRCLDMPID')['geometry'].count()\n",
    "# building_within_parcel_count.hist(bins=np.arange(0,10,1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17,9))\n",
    "\n",
    "ax.set_title(\"Histogram of APT to centroid distance on/not on BFP\")\n",
    "ax.set_xlabel(\"APT point to centroid distance(meters)\")\n",
    "ax.set_ylabel(\"counts\")\n",
    "\n",
    "frqTrue, edgesTrue = np.histogram(building_within_parcel_count, bins = np.arange(1,10,1))\n",
    "p1 = ax.bar(edgesTrue[:-1], frqTrue, width=np.diff(edgesTrue), edgecolor=\"black\", align=\"edge\",alpha=0.4,label='Address Points on Rooftop',color='orange')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildings_within_parcel(data: gpd.GeoSeries, count=None):\n",
    "    print(\"Acquiring BFP's within land Parcels \".format(count))\n",
    "    building_within_parcel_count = data.groupby('PRCLDMPID')['geometry'].count()\n",
    "    if count == 1:\n",
    "        parcel_ids_with_one_building = list(building_within_parcel_count[building_within_parcel_count == 1].keys())\n",
    "        filtered_dataframe = data[data['PRCLDMPID'].isin(parcel_ids_with_one_building)]\n",
    "    elif count == 2:\n",
    "        parcel_ids_with_two_buildings = list(building_within_parcel_count[building_within_parcel_count == 2].keys())\n",
    "        filtered_dataframe = data[data['PRCLDMPID'].isin(parcel_ids_with_two_buildings)]\n",
    "    else:\n",
    "        parcel_ids_with_n_buildings = list(building_within_parcel_count[building_within_parcel_count > 2].keys())\n",
    "        filtered_dataframe = data[data['PRCLDMPID'].isin(parcel_ids_with_n_buildings)]\n",
    "    return filtered_dataframe\n",
    "\n",
    "df_parcel_within_bfp = get_buildings_within_parcel(parcel_bfp_df, count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape\",df_parcel_within_bfp.shape)\n",
    "df_parcel_within_bfp.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcel_within_bfp = df_parcel_within_bfp.drop(['index_right','release'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Anchor point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parcel_anchorpoints(anchor_points_data,input_dataframe: gpd.GeoSeries):\n",
    "    print(\"Reading Anchor-Points data over Parcel-Building Geo-Dataframe\")\n",
    "    \n",
    "    anchorpoint_df = read_vector_data(anchor_points_data)\n",
    "    apt_df_columns = list(anchorpoint_df.columns)\n",
    "    \n",
    "    print(\"Processing Anchor-Points and Parcel-Building Geo-Dataframe\")\n",
    "    # find spatial join of input_dataframe with anchorpoint\n",
    "    grouped_df = gpd.sjoin(input_dataframe, anchorpoint_df, op='contains', how='inner')\n",
    "    print(grouped_df.keys())\n",
    "\n",
    "    def _get_apt_point(val):\n",
    "        return anchorpoint_df['geometry'].loc[val]\n",
    "\n",
    "    grouped_df['APT'] = grouped_df['index_right'].apply(lambda x: _get_apt_point(x))\n",
    "    grouped_df = grouped_df.drop(['index_right'], axis=1)\n",
    "    return grouped_df , apt_df_columns\n",
    "\n",
    "process_df , cols = get_parcel_anchorpoints(apt_data_path,df_parcel_within_bfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Required columns\",cols)\n",
    "process_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_point_on_polygon(bfp_polygon, apt_point):\n",
    "    flag = False\n",
    "    if bfp_polygon.contains(apt_point):\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "def multi_map(x,required_cols, area_thresh=150,bfp_per_parcel =2 ):\n",
    "    ret = dict()\n",
    "    req_columns = ['PRCLDMPID', 'building_roi', 'APT'] + required_cols\n",
    "    # get two buildings as we have selected \n",
    "    \n",
    "    \n",
    "    for cols in req_columns:\n",
    "        ret[cols] = x.iloc[0][cols]\n",
    "    \n",
    "    if bfp_per_parcel ==2:\n",
    "        building_polygons = list(x['building_roi'][:2])\n",
    "        geo_area = list(x['building_roi'].apply(lambda poly: abs(geod.geometry_area_perimeter(poly)[0])))[:bfp_per_parcel]\n",
    "        area_diff = geo_area[0] - geo_area[1]\n",
    "        \n",
    "        if area_diff > 0:\n",
    "            if area_diff > area_thresh:\n",
    "                ret['building_roi'] = building_polygons[0]\n",
    "            else:\n",
    "                ret['building_roi'] = (list(x['building_roi'][:bfp_per_parcel])[get_nearest_poly(list(x['APT'])[0], building_polygons)])\n",
    "\n",
    "        elif area_diff <= 0:\n",
    "            if np.abs(area_diff) > area_thresh:\n",
    "                ret['building_roi'] = building_polygons[1]\n",
    "            else:\n",
    "                ret['building_roi'] = (list(x['building_roi'][:bfp_per_parcel])[get_nearest_poly(list(x['APT'])[0], building_polygons)])\n",
    "    \n",
    "    if bfp_per_parcel>2:\n",
    "        building_polygons = list(x['building_roi'][:3])\n",
    "        mnr_apt = list(x['APT'])[0]\n",
    "        point_on_polygon = False\n",
    "        \n",
    "        for polygon in building_polygons:\n",
    "            if check_point_on_polygon(polygon,mnr_apt):\n",
    "                point_on_polygon=True \n",
    "        if not point_on_polygon:\n",
    "            ret['building_roi'] = (list(x['building_roi'][:3])[get_nearest_poly(mnr_apt, building_polygons)])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apt_to_bfp_distance(self, data):\n",
    "    anchor_point = data['APT']\n",
    "    bfp_centroid = data['updated_geometries']\n",
    "    return get_distance(anchor_point, bfp_centroid)\n",
    "\n",
    "complexity = 2 \n",
    "\n",
    "if complexity ==1 :\n",
    "    process_df['updated_geometries'] = process_df['building_roi'].apply(lambda x: x.centroid)\n",
    "    process_df['updated_dt'] = process_df.apply(lambda x: get_apt_to_bfp_distance(x),axis=1)\n",
    "\n",
    "# if complexity ==2 :\n",
    "#     process_df = process_df.groupby(['PRCLDMPID'], as_index=False).apply(lambda x: pd.Series(multi_map(x,required_cols=cols)))\n",
    "\n",
    "filter_df = process_df.groupby(['PRCLDMPID'], as_index=False).apply(lambda x: pd.Series(multi_map(x,required_cols=cols,bfp_per_parcel=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filter_df.shape)\n",
    "filter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pygeo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70ce54ad97a74852893fb3a2eed7fab00a48abaab9c8d77c9e0b6a3e4b45b116"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
