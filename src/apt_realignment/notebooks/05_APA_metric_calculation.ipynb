{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "176fd7bb-582c-4d57-94e0-2f46829e0dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Requirements + setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "858985c9-74ac-4335-8a1d-71e4d7335b14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ab241cd-8604-48f8-b9bd-8a6d9046a399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta : True\n",
      "/mnt/c/Users/tandon/OneDrive - TomTom/Desktop/tomtom/Workspace/01_Rooftop_accuracy/APT-Realignment/data/Texas/Final_Updated_geom_Texas.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union\n",
    "import folium\n",
    "from shapely.geometry import mapping\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import unidecode\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "from addressing.utils import libpostal\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from addressing.automatic_matching import automatic_matching\n",
    "from addressing.automatic_matching.rooftop.rooftop import haversine_distance\n",
    "import re\n",
    "import sys, os\n",
    "import sqlalchemy\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "data_path = \"/mnt/c/Users/tandon/OneDrive - TomTom/Desktop/tomtom/Workspace/01_Rooftop_accuracy/APT-Realignment/data/Texas\"\n",
    "state = \"Texas\"\n",
    "county = 'bexar'\n",
    "delta = True\n",
    "\n",
    "print(\"Delta :\",delta)\n",
    "\n",
    "Updated_geometries_pickle = os.path.join(data_path,'Final_Updated_geom_Texas.pkl')\n",
    "if os.path.isfile(Updated_geometries_pickle):\n",
    "    print(Updated_geometries_pickle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43075523-cc26-4f2a-9740-1f112992ebc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2dff75e-f270-48e1-b1ba-1bd0e0324e24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DB = {\n",
    "'host' : \"10.137.173.84\",\n",
    "'port' : '5432',\n",
    "'database' :  \"STAN\",\n",
    "'user' : \"strategicadmin\",\n",
    "'password' :  \"TBmG4Yj3DdwOI+Aq\"\n",
    "}\n",
    "\n",
    "class ReadAndWrite2PostgresDB:\n",
    "\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "\n",
    "    def read_from_db(self, query, retry_num=3):\n",
    "\n",
    "        for _ in range(retry_num):\n",
    "            df = None\n",
    "            try:\n",
    "                df = pd.read_sql(query, self.engine)\n",
    "                return df\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def write_to_db(self, df, schema, table_name, retry_num=3):\n",
    "\n",
    "        for _ in range(retry_num):\n",
    "            try:\n",
    "                \n",
    "                df.to_sql(\n",
    "                    table_name,\n",
    "                    con=self.engine,\n",
    "                    if_exists='append',\n",
    "                    schema=schema,\n",
    "                    index = False)\n",
    "                print(\"Table stored!\")\n",
    "                return 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80568a0b-e07d-49a6-a0ae-77315b33d048",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(f'postgresql+psycopg2://{DB[\"user\"]}:{DB[\"password\"]}@{DB[\"host\"]}:{DB[\"port\"]}/{DB[\"database\"]}',echo = False)\n",
    "raw2p = ReadAndWrite2PostgresDB(engine)\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98333593-68b7-4dec-be28-c85c20652e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "counties = 'cook', 'bexar', 'dallas', 'orange', 'maricopa', 'marion', 'tarrant', 'clark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e54f6e1d-86be-4d62-a156-438d31f1aa7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date = date.today()\n",
    "print(date)\n",
    "print(county)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d68d6dc1-0535-4ec2-a6be-73118aae00fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b863fd02-66ed-417d-a366-f3ab7997ec81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "countries_stopwords = {\n",
    "  'br': stopwords.words('portuguese') + ['rua', 'avenida'], \n",
    "  'ca': stopwords.words('french') + stopwords.words('english') +  ['road', 'street', 'st.', 'st', 'rue', 'chemin', 'avenue'],\n",
    "  'es': stopwords.words('spanish') + ['calle', 'avenida', 'callejón', 'paseo'],\n",
    "  'fr': stopwords.words('french') + ['rue', 'chemin', 'avenue'],\n",
    "  'gb': stopwords.words('english') + ['street', 'road', 'avenue', 'st.', 'st', 'drive'],\n",
    "  'it': stopwords.words('italian') + ['via', 'viale', 'strada'],\n",
    "  'mx': stopwords.words('spanish') + ['calle', 'avenida', 'callejón', 'paseo'],\n",
    "  'us': stopwords.words('english') + ['street', 'road', 'avenue', 'st.', 'st', 'drive'],\n",
    "  'be': stopwords.words('french') + ['rue', 'chemin', 'avenue'],\n",
    "  'za': stopwords.words('english') + ['street', 'road', 'avenue', 'st.', 'st', 'drive']\n",
    "}\n",
    "\n",
    "countries_stopwords = {k:'|'.join(['\\\\b' + word + '\\\\b' for word in v]) for k, v in countries_stopwords.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b72b52ac-0082-473e-a987-97ca6e3a81d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed8deeee-d104-4a26-a188-fe0a397348b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sample addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc241cdc-5231-498e-bc8c-f140a59463cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_query = f\"\"\"SELECT * FROM \"STAN_169\".sample where county = '{county}'\"\"\"\n",
    "sample_df = raw2p.read_from_db(query = sample_query)\n",
    "sample_geom = gpd.GeoSeries.from_wkt(sample_df.geometry)\n",
    "sample_gdf = gpd.GeoDataFrame(sample_df.drop('geometry', axis = 1),\n",
    "                                geometry = sample_geom,\n",
    "                                crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69f812f8-3d69-40c5-9e1f-2d6325269df3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1029, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>searched_query_unidecode_sample</th>\n",
       "      <th>libpostal_query</th>\n",
       "      <th>libpostal_response</th>\n",
       "      <th>libpostal_house</th>\n",
       "      <th>libpostal_category</th>\n",
       "      <th>libpostal_near</th>\n",
       "      <th>libpostal_house_number</th>\n",
       "      <th>libpostal_road</th>\n",
       "      <th>libpostal_unit</th>\n",
       "      <th>...</th>\n",
       "      <th>libpostal_state_district</th>\n",
       "      <th>libpostal_state</th>\n",
       "      <th>libpostal_country_region</th>\n",
       "      <th>libpostal_country</th>\n",
       "      <th>libpostal_world_region</th>\n",
       "      <th>lat_sample</th>\n",
       "      <th>lon_sample</th>\n",
       "      <th>county</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>4426 N Hein Rd, San Antonio, TX 78220, USA</td>\n",
       "      <td>{\"query\": \"4426 N Hein Rd, San Antonio, TX 782...</td>\n",
       "      <td>{'country': 'usa', 'city': 'san antonio', 'roa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4426</td>\n",
       "      <td>n hein rd</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>tx</td>\n",
       "      <td></td>\n",
       "      <td>usa</td>\n",
       "      <td></td>\n",
       "      <td>29.412026</td>\n",
       "      <td>-98.406943</td>\n",
       "      <td>bexar</td>\n",
       "      <td>6232ext2</td>\n",
       "      <td>POLYGON ((-98.39796 29.41203, -98.39800 29.411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>21630 Milsa Dr, San Antonio, TX 78256, USA</td>\n",
       "      <td>{\"query\": \"21630 Milsa Dr, San Antonio, TX 782...</td>\n",
       "      <td>{'country': 'usa', 'city': 'san antonio', 'roa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21630</td>\n",
       "      <td>milsa dr</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>tx</td>\n",
       "      <td></td>\n",
       "      <td>usa</td>\n",
       "      <td></td>\n",
       "      <td>29.642222</td>\n",
       "      <td>-98.622913</td>\n",
       "      <td>bexar</td>\n",
       "      <td>6233ext2</td>\n",
       "      <td>POLYGON ((-98.61393 29.64222, -98.61397 29.641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA</td>\n",
       "      <td>8227 Wayside Creek, San Antonio, TX 78255, USA</td>\n",
       "      <td>{\"query\": \"8227 Wayside Creek, San Antonio, TX...</td>\n",
       "      <td>{'country': 'usa', 'city': 'san antonio', 'roa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8227</td>\n",
       "      <td>wayside creek</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>tx</td>\n",
       "      <td></td>\n",
       "      <td>usa</td>\n",
       "      <td></td>\n",
       "      <td>29.671692</td>\n",
       "      <td>-98.646167</td>\n",
       "      <td>bexar</td>\n",
       "      <td>6234ext2</td>\n",
       "      <td>POLYGON ((-98.63718 29.67169, -98.63723 29.670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA</td>\n",
       "      <td>130A Interloop Rd, San Antonio, TX 78216, USA</td>\n",
       "      <td>{\"query\": \"130A Interloop Rd, San Antonio, TX ...</td>\n",
       "      <td>{'country': 'usa', 'city': 'san antonio', 'roa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>130a</td>\n",
       "      <td>interloop rd</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>tx</td>\n",
       "      <td></td>\n",
       "      <td>usa</td>\n",
       "      <td></td>\n",
       "      <td>29.493616</td>\n",
       "      <td>-98.502956</td>\n",
       "      <td>bexar</td>\n",
       "      <td>6235ext2</td>\n",
       "      <td>POLYGON ((-98.49397 29.49362, -98.49402 29.492...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA</td>\n",
       "      <td>612 Devine St, San Antonio, TX 78210, USA</td>\n",
       "      <td>{\"query\": \"612 Devine St, San Antonio, TX 7821...</td>\n",
       "      <td>{'country': 'usa', 'city': 'san antonio', 'roa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>612</td>\n",
       "      <td>devine st</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>tx</td>\n",
       "      <td></td>\n",
       "      <td>usa</td>\n",
       "      <td></td>\n",
       "      <td>29.408747</td>\n",
       "      <td>-98.480484</td>\n",
       "      <td>bexar</td>\n",
       "      <td>6236ext2</td>\n",
       "      <td>POLYGON ((-98.47150 29.40875, -98.47154 29.407...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                 searched_query_unidecode_sample  \\\n",
       "0     USA      4426 N Hein Rd, San Antonio, TX 78220, USA   \n",
       "1     USA      21630 Milsa Dr, San Antonio, TX 78256, USA   \n",
       "2     USA  8227 Wayside Creek, San Antonio, TX 78255, USA   \n",
       "3     USA   130A Interloop Rd, San Antonio, TX 78216, USA   \n",
       "4     USA       612 Devine St, San Antonio, TX 78210, USA   \n",
       "\n",
       "                                     libpostal_query  \\\n",
       "0  {\"query\": \"4426 N Hein Rd, San Antonio, TX 782...   \n",
       "1  {\"query\": \"21630 Milsa Dr, San Antonio, TX 782...   \n",
       "2  {\"query\": \"8227 Wayside Creek, San Antonio, TX...   \n",
       "3  {\"query\": \"130A Interloop Rd, San Antonio, TX ...   \n",
       "4  {\"query\": \"612 Devine St, San Antonio, TX 7821...   \n",
       "\n",
       "                                  libpostal_response libpostal_house  \\\n",
       "0  {'country': 'usa', 'city': 'san antonio', 'roa...                   \n",
       "1  {'country': 'usa', 'city': 'san antonio', 'roa...                   \n",
       "2  {'country': 'usa', 'city': 'san antonio', 'roa...                   \n",
       "3  {'country': 'usa', 'city': 'san antonio', 'roa...                   \n",
       "4  {'country': 'usa', 'city': 'san antonio', 'roa...                   \n",
       "\n",
       "  libpostal_category libpostal_near libpostal_house_number libpostal_road  \\\n",
       "0                                                     4426      n hein rd   \n",
       "1                                                    21630       milsa dr   \n",
       "2                                                     8227  wayside creek   \n",
       "3                                                     130a   interloop rd   \n",
       "4                                                      612      devine st   \n",
       "\n",
       "  libpostal_unit  ... libpostal_state_district libpostal_state  \\\n",
       "0                 ...                                       tx   \n",
       "1                 ...                                       tx   \n",
       "2                 ...                                       tx   \n",
       "3                 ...                                       tx   \n",
       "4                 ...                                       tx   \n",
       "\n",
       "  libpostal_country_region libpostal_country libpostal_world_region  \\\n",
       "0                                        usa                          \n",
       "1                                        usa                          \n",
       "2                                        usa                          \n",
       "3                                        usa                          \n",
       "4                                        usa                          \n",
       "\n",
       "  lat_sample lon_sample county sample_id  \\\n",
       "0  29.412026 -98.406943  bexar  6232ext2   \n",
       "1  29.642222 -98.622913  bexar  6233ext2   \n",
       "2  29.671692 -98.646167  bexar  6234ext2   \n",
       "3  29.493616 -98.502956  bexar  6235ext2   \n",
       "4  29.408747 -98.480484  bexar  6236ext2   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-98.39796 29.41203, -98.39800 29.411...  \n",
       "1  POLYGON ((-98.61393 29.64222, -98.61397 29.641...  \n",
       "2  POLYGON ((-98.63718 29.67169, -98.63723 29.670...  \n",
       "3  POLYGON ((-98.49397 29.49362, -98.49402 29.492...  \n",
       "4  POLYGON ((-98.47150 29.40875, -98.47154 29.407...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sample_gdf.shape)\n",
    "sample_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38b9a4e5-4199-4a8d-9602-dc6b65b8213f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05a6ab54-3fee-4f88-97f5-675946d73c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_query = f\"\"\"SELECT * FROM \"STAN_169\".source_v0 where county = '{county}'\"\"\"\n",
    "source_df = raw2p.read_from_db(query = source_query)\n",
    "source_geom = gpd.GeoSeries.from_wkt(source_df.geometry)\n",
    "source_gdf = gpd.GeoDataFrame(source_df.drop('geometry', axis = 1),\n",
    "                                geometry = source_geom,\n",
    "                                crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_id</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>hsn</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>street_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>prefix</th>\n",
       "      <th>suffix</th>\n",
       "      <th>pre_dir</th>\n",
       "      <th>post_dir</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>county</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005554-3400-2800-0000-000000b7a84b</td>\n",
       "      <td>78245</td>\n",
       "      <td>2922</td>\n",
       "      <td>TX</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Amber Glade</td>\n",
       "      <td>USA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-98.678005</td>\n",
       "      <td>29.401356</td>\n",
       "      <td>bexar</td>\n",
       "      <td>POINT (-98.67800 29.40136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ebf20abd-d425-4b88-a049-4acd9df6ca5e</td>\n",
       "      <td>78227</td>\n",
       "      <td>174</td>\n",
       "      <td>TX</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Ray Ellison</td>\n",
       "      <td>USA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-98.643672</td>\n",
       "      <td>29.368750</td>\n",
       "      <td>bexar</td>\n",
       "      <td>POINT (-98.64367 29.36875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00005554-3400-2800-0000-000000b883a4</td>\n",
       "      <td>78250</td>\n",
       "      <td>7447</td>\n",
       "      <td>TX</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Branston</td>\n",
       "      <td>USA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-98.690044</td>\n",
       "      <td>29.511031</td>\n",
       "      <td>bexar</td>\n",
       "      <td>POINT (-98.69004 29.51103)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005554-3400-2800-0000-000000baaf03</td>\n",
       "      <td>78223</td>\n",
       "      <td>311</td>\n",
       "      <td>TX</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Kate Schenck Ave</td>\n",
       "      <td>USA</td>\n",
       "      <td>None</td>\n",
       "      <td>Ave</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-98.430875</td>\n",
       "      <td>29.361513</td>\n",
       "      <td>bexar</td>\n",
       "      <td>POINT (-98.43088 29.36151)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005554-3400-2800-0000-000000ba28b3</td>\n",
       "      <td>78212</td>\n",
       "      <td>1706</td>\n",
       "      <td>TX</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>McCullough Ave</td>\n",
       "      <td>USA</td>\n",
       "      <td>None</td>\n",
       "      <td>Ave</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-98.491592</td>\n",
       "      <td>29.445320</td>\n",
       "      <td>bexar</td>\n",
       "      <td>POINT (-98.49159 29.44532)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                feat_id postal_code   hsn state         city  \\\n",
       "0  00005554-3400-2800-0000-000000b7a84b       78245  2922    TX  San Antonio   \n",
       "1  ebf20abd-d425-4b88-a049-4acd9df6ca5e       78227   174    TX  San Antonio   \n",
       "2  00005554-3400-2800-0000-000000b883a4       78250  7447    TX  San Antonio   \n",
       "3  00005554-3400-2800-0000-000000baaf03       78223   311    TX  San Antonio   \n",
       "4  00005554-3400-2800-0000-000000ba28b3       78212  1706    TX  San Antonio   \n",
       "\n",
       "        street_name country_code prefix suffix pre_dir post_dir          x  \\\n",
       "0       Amber Glade          USA   None   None    None     None -98.678005   \n",
       "1       Ray Ellison          USA   None   None    None     None -98.643672   \n",
       "2          Branston          USA   None   None    None     None -98.690044   \n",
       "3  Kate Schenck Ave          USA   None    Ave    None     None -98.430875   \n",
       "4    McCullough Ave          USA   None    Ave    None     None -98.491592   \n",
       "\n",
       "           y county                    geometry  \n",
       "0  29.401356  bexar  POINT (-98.67800 29.40136)  \n",
       "1  29.368750  bexar  POINT (-98.64367 29.36875)  \n",
       "2  29.511031  bexar  POINT (-98.69004 29.51103)  \n",
       "3  29.361513  bexar  POINT (-98.43088 29.36151)  \n",
       "4  29.445320  bexar  POINT (-98.49159 29.44532)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0e7eb53-b3e5-498f-827b-b5514936c193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# gdf = gpd.GeoDataFrame(source_gdf[['feat_id','x','y','county','geometry']], crs=\"epsg:4326\", geometry='geometry')\n",
    "# gdf.to_crs(\"epsg:4326\")\n",
    "# gdf.to_file(os.path.join(data_path,state,county,'Apt_realignment_MSFT_OSM/source_df.shp'), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bba06ae-596c-4ca1-a4dd-037416be2645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# source_df = pd.read_pickle(f'wrang_source_v0_{county}.pickle')\n",
    "# source_gdf = gpd.GeoDataFrame(source_df, geometry = 'geometry', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c167c72f-778d-4eef-bb8a-1983b5184d54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ingesting Deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "385e81a0-2b95-4c54-9cd5-bf1cab93c924",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here goes the table of new changes to make to coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84345018-efdc-4c03-9c2b-9a9e553c9ee8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELTA for updating geometries is  True\n"
     ]
    }
   ],
   "source": [
    "print(\"DELTA for updating geometries is \",delta)\n",
    "if delta:\n",
    "    delta_table_read = pd.read_pickle(Updated_geometries_pickle)\n",
    "    delta_table_read = delta_table_read[['feat_id', 'updated_geometries']]\n",
    "    delta_table_read['datetime_version'] = pd.Timestamp.now(tz = 'utc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6889997, 3)\n"
     ]
    }
   ],
   "source": [
    "if delta:\n",
    "    print(delta_table_read.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f71becfd-6382-4a49-b85c-61f122e7b37a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    delta_table_write = delta_table_read[['feat_id', 'updated_geometries', 'datetime_version']]\n",
    "    delta_table_write['updated_geometries'] = delta_table_write['updated_geometries'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "156b32d0-22ff-4543-9f31-27ae0db1313f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    raw2p.write_to_db(df = delta_table_write, schema = 'STAN_169', table_name = 'delta_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f8d87441-7f3c-431b-b6c5-691cd0269839",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14aee02a-6f08-47df-a700-bb8279c21699",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:  \n",
    "  delta_query = f\"\"\"\n",
    "  SELECT * FROM \"STAN_169\".delta_table where county = '{county}'\n",
    "  \"\"\"\n",
    "  delta_df = raw2p.read_from_db(query = delta_query)\n",
    "  delta_geom = gpd.GeoSeries.from_wkt(delta_df.updated_geometries)\n",
    "  delta_gdf = gpd.GeoDataFrame(delta_df.drop('updated_geometries', axis = 1),\n",
    "                                  geometry = delta_geom,\n",
    "                                  crs = 'EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "22832e61-c735-4e91-b27f-c31b33a53218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "  delta_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28f59e8f-1c65-4d74-80f4-f5d8e4852636",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Replacing Delta Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "342a1c5f-c164-4e5b-8e1a-afa4aa13e282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_geometries(source_gdf, delta_gdf):\n",
    "    '''\n",
    "    Takes a 'source' geodataframe - copy of MNR database, a 'delta' geodataframe and replaces, for every APT (key: feat_id) in source dataframe, the \n",
    "    coordinates in the sorce with the coordinates in the new \n",
    "\n",
    "            Parameters:\n",
    "                    source_gdf (gpd.GeoDataFrame): geodataframe containing MNR coordinates for every APT (feat_id)\n",
    "                    delta_gdf (gpd.GeoDataFrame): geodataframe containing NEW coordinates for some APTs (feat_id)\n",
    "\n",
    "            Returns:\n",
    "                    source_gdf_new (gpd.GeoDataFrame): geodataframe containing MNR information for APT but with new coordinates\n",
    "    '''\n",
    "    # \n",
    "    delta_gdf_grouped = delta_gdf[delta_gdf.groupby('feat_id').datetime_version.transform('max') == delta_gdf.datetime_version]\n",
    "\n",
    "    source_gdf_new = source_gdf.merge(delta_gdf_grouped[[\"feat_id\", \"geometry\"]], on=\"feat_id\", how=\"left\")\n",
    "\n",
    "    source_gdf_new.loc[~source_gdf_new.geometry_y.isna(), \"geometry_x\"] = source_gdf_new.loc[~source_gdf_new.geometry_y.isna(), \"geometry_y\"]\n",
    "\n",
    "    source_gdf_new = source_gdf_new.drop([\"geometry_y\"], axis=1).rename({\"geometry_x\": \"geometry\"}, axis = 1)\n",
    "\n",
    "    source_gdf_new = gpd.GeoDataFrame(source_gdf_new.drop('geometry', axis = 1), \n",
    "                                    geometry = source_gdf_new.geometry, crs = 'EPSG:4326')\n",
    "\n",
    "    return source_gdf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f902d2d-8f06-4e71-834f-b007231e82dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    source_delta = replace_geometries(source_gdf, delta_gdf)\n",
    "    source_delta_gdf = gpd.GeoDataFrame(source_delta.drop('geometry', axis = 1), geometry = source_delta.geometry, crs = 'EPSG:4326')\n",
    "    source_delta_gdf.x = source_delta_gdf.geometry.apply(lambda p: p.x)\n",
    "    source_delta_gdf.y = source_delta_gdf.geometry.apply(lambda p: p.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33574bea-4522-48e7-9af6-8d02e0e09ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Joining Sample and Source - Spatial Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc64a8de-1d19-49eb-b919-7d4594df423a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "  joined_sample = source_delta_gdf.sjoin(sample_gdf, how='right', predicate='intersects')\n",
    "else:\n",
    "  joined_sample = source_gdf.sjoin(sample_gdf, how = 'right', predicate = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3fd6e3d-4a06-48ea-8517-9b8970c063ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8549df1e-dc2c-4509-8858-c311093375c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if delta: \n",
    "#   del source_gdf\n",
    "#   del delta_gdf\n",
    "# else:\n",
    "#   del source_delta_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50776690-dbe4-4161-8c7b-e8ca6db7953b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_sample.rename({\n",
    "    'hsn': 'hsnum',\n",
    "    'street_name': 'st_name',\n",
    "    'postal_code': 'zip_code'\n",
    "},\n",
    "axis = 1, \n",
    "inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96bc118c-eb99-43e8-9af8-500bcddea5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Parsing Joined Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7dd6d47a-6a09-4481-a0b6-d5774d8e91bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_joined_sample(spatial_joined_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Function inversely parses the addresses to create a searched query format so that the addresses in the source \n",
    "    can be compared to the addresses in the sample.\n",
    "\n",
    "    :param spatial_joined_df: DataFrame that contains the addresses from the source that are within the polygon of \n",
    "    the sample generated. It must contain the columns: ['hsn', 'unit_type', 'unit_num', 'pre_dir', 'prefix', 'suffix'\n",
    "    'post_dir', 'city', 'state', 'zip_code']\n",
    "    :type spatial_joined_df: pd.DataFrame\n",
    "    :return: The same dataframe with a column that contains the full addresses inversely parsed.\n",
    "    :rtype: pd.DataFrame\n",
    "    '''\n",
    "\n",
    "    df = spatial_joined_df.copy()\n",
    "\n",
    "    dict_of_columns = {\n",
    "        'hsnum': ' ', 'pre_dir': ' ', 'st_name': ' ', 'suffix': ', ', 'city': ' ', 'state': ' ', 'zip_code': ', ', 'country': ''\n",
    "    }\n",
    "    df['pre_dir'].fillna('', inplace=True)\n",
    "    df['prefix'].fillna('', inplace=True)\n",
    "    df['suffix'].fillna('', inplace=True)\n",
    "    df['post_dir'].fillna('', inplace=True)\n",
    "\n",
    "    for column in dict_of_columns.keys():\n",
    "\n",
    "        df[column + '_modified'] = df[column].astype(str) + dict_of_columns[column]\n",
    "\n",
    "    list_of_modified_columns = [col for col in df.columns if '_modified' in col]\n",
    "\n",
    "    df['searched_query'] = df[list_of_modified_columns].sum(axis=1)\n",
    "\n",
    "    df['street_name'] = df['pre_dir'] + ' ' + df['prefix'] + ' ' + df['st_name'] + ' ' + df['suffix'] + ' ' + df['post_dir']\n",
    "    df['name'] = '' #df['state']\n",
    "\n",
    "    df = df.rename(columns={'hsnum': 'hsn', 'searched_query': 'address', 'zip_code': 'postal_code', 'city': 'place_name', 'y': 'lat', 'x': 'lon'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08624024-3a9d-4461-b506-00f9b3031597",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = parse_joined_sample(joined_sample)\n",
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "752e86ec-e2fa-4244-8d43-c5e5ad61806c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del joined_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b0421aa-4025-4ff1-9dcb-09d177874c55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Matching Adresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f221a505-b239-4e9a-912d-a64df5ebf212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apt_similarity_filter(\n",
    "    #country:str,\n",
    "    df:pd.DataFrame,\n",
    "    sample_df:pd.DataFrame,\n",
    "    stopwords_pattern: str = '') -> pd.DataFrame:\n",
    "    \"\"\"Performs matching after making call in a given radius\n",
    "\n",
    "    :param country: country to call in MNR\n",
    "    :type country: str\n",
    "    :param df: DataFrame containing the sample addresses (must have coordinates)\n",
    "    :type df: pd.DataFrame\n",
    "    :param sample_df: DataFrame containing libpostal components for sample (df) addresses\n",
    "    :type sample_df: pd.DataFrame\n",
    "    :param radius: radius of the buffer\n",
    "    :type radius: float\n",
    "    :param inner_radius: radius in meters of a smaller buffer. When bigger than zero, we are essentially getting the point in a disk, defaults to 0\n",
    "    :type inner_radius: int or float, optional\n",
    "    :param stopwords_pattern: regex pattern to remove stopwords, if needed. Optional, defaults to None\n",
    "    :type stopwords_pattern: str\n",
    "    :return: DataFrame with the APTs that matched\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    apts_df = df.copy()\n",
    "\n",
    "    # Fill NAs\n",
    "    apts_df[['address', 'street_name', 'hsn', 'postal_code',\n",
    "                    'place_name', 'name']] = apts_df[['address', 'street_name', 'hsn',\n",
    "                                                                            'postal_code', 'place_name', 'name']].fillna('')\n",
    "\n",
    "    # Drop duplicates\n",
    "    #apts_df = apts_df.drop_duplicates(['searched_query', 'address']).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Create extra columns for stopwords, optional unidecode \n",
    "    cols_stopwords = ['address', 'street_name', 'place_name']\n",
    "    for col in cols_stopwords:\n",
    "        col_create = col + '_no_stopwords'\n",
    "        apts_df[col_create] =  apts_df[col].str.replace(stopwords_pattern, '', case=False, regex=True)\n",
    "        \n",
    "    for col in cols_stopwords:\n",
    "        col_create = col + '_no_stopwords_unidecode'\n",
    "        apts_df[col_create] =  apts_df[col+'_no_stopwords'].apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "    \n",
    "    # Merge to APTs\n",
    "    #apts_df = apts_df.merge(sample_df.drop(columns=['country', 'searched_query_unidecode_sample']),\n",
    "    #                                      how='left', \n",
    "    #                                      on=['searched_query'])\n",
    "    apts_df['libpostal_road_no_stopwords'] = apts_df.libpostal_road.str.replace(stopwords_pattern, '', case=False, regex=True)\n",
    "\n",
    "\n",
    "    # House number similarity: filter obvious non matches\n",
    "    apts_df['hsn_similarity'] = list(map(fuzz.token_set_ratio, apts_df.libpostal_house_number, apts_df.hsn))\n",
    "    apts_df['re_pattern'] = '\\\\b' + apts_df.hsn.astype(str) + '\\\\b'\n",
    "    #apts_df['hsn_in_query'] = apts_df.apply(lambda x: bool(re.search(x.re_pattern, x.searched_query_unidecode_sample)), axis=1)\n",
    "    #apts_df['hsn_similarity'] = np.where((apts_df.hsn_in_query), 100, apts_df.hsn_similarity)\n",
    "\n",
    "    dropped_df = apts_df.loc[apts_df.hsn_similarity <= 60].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    apts_df = apts_df.loc[apts_df.hsn_similarity > 60].reset_index(drop=True)\n",
    "\n",
    "    # Postal code similarity\n",
    "    apts_df['postcode_similarity'] = list(map(fuzz.WRatio, \n",
    "                                                     apts_df.libpostal_postcode, \n",
    "                                                     apts_df.postal_code.fillna('').astype(str)))\n",
    "    apts_df['postcode_similarity'] = np.where(apts_df.libpostal_postcode=='', np.nan,\n",
    "                                                     np.where(apts_df.postal_code=='', 50, apts_df.postcode_similarity))\n",
    "\n",
    "    \n",
    "    # Road similarity\n",
    "    apts_df['road_similarity'] = list(map(fuzz.token_set_ratio, \n",
    "                                                 apts_df.libpostal_road_no_stopwords, \n",
    "                                                 apts_df.street_name_no_stopwords))\n",
    "    apts_df['road_similarity_unidecode'] = list(map(fuzz.token_set_ratio, \n",
    "                                                           apts_df.libpostal_road_no_stopwords, \n",
    "                                                           apts_df.street_name_no_stopwords_unidecode)) \n",
    "    apts_df['road_similarity'] = apts_df[['road_similarity', 'road_similarity_unidecode']].max(axis=1)\n",
    "    \n",
    "    # Locality similarity\n",
    "    apts_df['searched_query_tokens'] = (apts_df.libpostal_road.astype(str) + ' ' + \n",
    "                                               apts_df.libpostal_house_number.astype(str) + ' ' + \n",
    "                                               apts_df.libpostal_postcode.astype(str))\n",
    "    \n",
    "    apts_df['provider_tokens'] = (apts_df.street_name.astype(str) + ' ' + \n",
    "                                         apts_df.hsn.astype(str) + ' ' + apts_df.postal_code.astype(str))\n",
    "    apts_df['aux_searched_query'] = apts_df.apply(lambda x: automatic_matching.replace_tokens(x.searched_query_unidecode_sample, x.searched_query_tokens), axis=1)\n",
    "    apts_df['aux_provider_address'] = apts_df.apply(lambda x: automatic_matching.replace_tokens(x.address, x.provider_tokens), axis=1)\n",
    "    apts_df['aux_provider_address'] = apts_df.aux_provider_address.fillna('').apply(lambda x: unidecode.unidecode(x))\n",
    "    apts_df['locality_wratio'] = apts_df.apply(lambda x: fuzz.WRatio(str(x.aux_searched_query).lower(), str(x.aux_provider_address).lower()), axis=1)\n",
    "    apts_df['locality_city_state_ratio'] = apts_df.apply(lambda x: fuzz.WRatio(str(x.libpostal_city) + ' ' + str(x.libpostal_state),\n",
    "                                                                                            str(x.place_name) + ' ' + str(x.name)), axis=1)\n",
    "    apts_df['locality_similarity'] = apts_df[['locality_wratio', 'locality_city_state_ratio']].mean(axis=1)\n",
    "\n",
    "    apts_df['mnr_query_distance'] = apts_df.apply(lambda x: haversine_distance(x.lat, x.lon,\n",
    "                                                                                               x.lat_sample, x.lon_sample)\n",
    "                                                                  if not np.isnan(x.lat) else 1e7\n",
    "                                                                  , axis=1)\n",
    "\n",
    "    # Compute mean similarity\n",
    "    apts_df['mean_similarity'] = (apts_df[['locality_similarity', 'hsn_similarity', \n",
    "                                                         'postcode_similarity', 'road_similarity']].mean(axis=1)\n",
    "                                        * np.where(apts_df.hsn_similarity >= 70 , 1, 0)\n",
    "                                        * np.where(apts_df.road_similarity >= 60 , 1, 0)  \n",
    "                                        * np.where(apts_df.mnr_query_distance > 1000, 0, 1)\n",
    "                                        )\n",
    "\n",
    "\n",
    "    apts_df_matching = (\n",
    "        apts_df.sort_values(by='mnr_query_distance')\n",
    "        .loc[apts_df.groupby(['sample_id'])\n",
    "        .mean_similarity.idxmax()]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute matching\n",
    "    apts_df_matching['match'] = pd.NaT\n",
    "    \n",
    "    apts_df_matching['match'] = np.where(apts_df_matching.mean_similarity >= 70, 1, pd.NaT) #90 so far best\n",
    "\n",
    "    #address_matches = apts_df_matching['searched_query_unidecode_sample']\n",
    "    address_sample_ids = apts_df_matching['sample_id']\n",
    "    \n",
    "    #non_matches = dropped_df[~dropped_df['searched_query_unidecode_sample'].isin(address_matches)]\n",
    "    non_matches_ids = dropped_df[~dropped_df['sample_id'].isin(address_sample_ids)]\n",
    "    \n",
    "    #addresses_to_add = non_matches['searched_query_unidecode_sample'].unique()\n",
    "    addresses_to_add_ids = non_matches_ids['sample_id'].unique()\n",
    "     \n",
    "    addresses_id_df = pd.DataFrame(\n",
    "        {'sample_id': addresses_to_add_ids, 'match': [pd.NaT] * len(addresses_to_add_ids)}\n",
    "    )\n",
    "    \n",
    "    addresses_id_df = addresses_id_df.merge(sample_df[['sample_id', 'searched_query_unidecode_sample']], on = 'sample_id', how = 'left')\n",
    "    \n",
    "    cols_to_add = [col for col in apts_df_matching if col not in addresses_id_df.columns]\n",
    "    \n",
    "    addresses_id_df.loc[:, cols_to_add] = ''\n",
    "    addresses_id_df_reordered = addresses_id_df[apts_df_matching.columns]\n",
    "    \n",
    "    apts_final = pd.concat([apts_df_matching, addresses_id_df_reordered])\n",
    "    \n",
    "\n",
    "    return apts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cbeaf40-fad6-486a-aea6-dfbd37ad74fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_df = apt_similarity_filter(df = parsed_df, sample_df = sample_gdf, stopwords_pattern = countries_stopwords.get('us'))\n",
    "del sample_gdf\n",
    "del parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68943219-58f1-488f-9fa4-2b9f80e78dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_df['match'] = similarity_df['match'].fillna(0)\n",
    "\n",
    "match_proportion = np.mean(similarity_df['match'])\n",
    "clean_proportion = round(match_proportion * 100, 2)\n",
    "print(f'The proportion of matches is: {clean_proportion}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3ab8acb-fc89-44ff-9651-ffda7f50f5cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(similarity_df.shape)\n",
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1f3bc5e-a0ab-4080-92b7-2f5ee638a020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df = similarity_df[['feat_id', 'match', 'sample_id']]\n",
    "match_df['county'] = county\n",
    "match_df['datetime_run'] = pd.Timestamp.now(tz = 'utc')\n",
    "match_df.rename({'match': 'asf'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "771e742f-f94a-4f4c-b3df-c1760ee2e78d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"shape of the data\",match_df.shape)\n",
    "match_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eaaf66e5-d25c-494f-be60-d4e3938a8b6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c31b4b5d-dfe2-4ee4-94ba-a462c7818f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pylab\n",
    "\n",
    "def bootstrap_resample(df, agg_fun, times=1000, seed=0):\n",
    "    reboot = []\n",
    "    \n",
    "    for t in range(times):\n",
    "        df_boot = df.sample(frac = 1, replace=True, random_state = t+seed)\n",
    "        reboot.append(agg_fun(df_boot))\n",
    "    draw_qqplot(reboot)\n",
    "    return reboot\n",
    "\n",
    "def draw_qqplot(reboot):\n",
    "    data = np.asarray(reboot)\n",
    "    # sm.ProbPlot(data)\n",
    "    sm.qqplot(data, line='45')\n",
    "    pylab.show()\n",
    "\n",
    "\n",
    "def percentile_bootstrap(df, agg_fun, conf=0.95, times=1000, seed=0):\n",
    "    \"\"\"Generic Percentile Bootstrap\n",
    "    This function returns a percentile bootstrap confidence interval for a statistic.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with the observed random vectors. Each row represents an observation an each column is a random variable.\n",
    "        agg_fun (function): Aggregation function. This function should receive as input a pandas.DataFrame (resamples) and return a \n",
    "        number with the computed statistic.\n",
    "        conf (float, optional): Confidence level of the returned interval. Defaults to 0.9.\n",
    "        times (int, optional): Bootstrap resamples. Defaults to 1000.\n",
    "        seed (int, optional): Random seed. Defaults to 0.\n",
    "    Returns:\n",
    "        numpy.array: Percentile Boostrap CI [lower, upper]\n",
    "    \"\"\"    \n",
    "    reboot = bootstrap_resample(df, agg_fun, times, seed)\n",
    "    return np.quantile(reboot, [(1-conf)/2, (1-conf)/2+conf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot normalized Similarity data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yeojohnson\n",
    "\n",
    "norm_data,_ = yeojohnson(similarity_df['match'])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.displot(norm_data, kind = \"kde\",color = \"#e64e4e\", height=10, aspect=2,\n",
    "            linewidth = 5 )\n",
    "ax.fig.suptitle('Data distribution', size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e706568-8bc9-495d-b10c-6fe0a9708f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[lower_distance, upper_distance] = percentile_bootstrap(similarity_df['match'], np.mean)\n",
    "\n",
    "# print(lower_distance,upper_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f478813-2e89-46b6-9981-89308bc8020c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    version = date\n",
    "else:\n",
    "    version = 'New_benchmark'\n",
    "\n",
    "results_sum = pd.DataFrame(\n",
    "    data=[[lower_distance, match_proportion, upper_distance, '%', 'ASF', version, county]], \n",
    "    columns=['lower_bound', 'calculated_metric', 'upper_bound', 'units', 'metric', 'version', 'county'], index = None)\n",
    "results_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06fbf9e4-eced-4042-a47b-fd54fb0bddd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Positional Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9474b1bb-5213-4b65-aec5-1613eba557ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 90th percentile\n",
    "\n",
    "We consider the 90th percentile of the distance of matches as a metric for the Positional Accuracy. The distance we obtain below is be the distance for which 90% of the data is lower. The interesting thing about this metric is that it's expressed in terms of distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "987e0e6f-69f0-4215-88db-9bb450cb616a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matches_df = similarity_df[similarity_df['match'] == 1]\n",
    "\n",
    "matches_df['mnr_query_distance'] = matches_df['mnr_query_distance'].astype(float)\n",
    "\n",
    "del similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot normalized MNr Query Distance match Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yeojohnson\n",
    "\n",
    "norm_data,_ = yeojohnson(matches_df['mnr_query_distance'])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.displot(norm_data, kind = \"kde\",color = \"#e64e4e\", height=10, aspect=2,\n",
    "            linewidth = 5 )\n",
    "ax.fig.suptitle('Data distribution', size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21eb97a7-e012-41e5-b689-f6af90b310c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "positional_accuracy_distance = round(np.quantile(matches_df['mnr_query_distance'], 0.9), 2)\n",
    "print(f'Positional Accuracy (90th percentile distance) is: {positional_accuracy_distance}m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44b4ba00-126c-44e9-b913-96d508d4615b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea9d7f24-1811-4d3b-b126-b13a1a59eee8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[lower_percentile90, upper_percentile90] = percentile_bootstrap(\n",
    "    matches_df['mnr_query_distance'], lambda x: np.quantile(x, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e69b1334-2741-4d14-aa3b-2cb9eb3d6d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    version = date\n",
    "else:\n",
    "    version = 'New_benchmark'\n",
    "\n",
    "new_result = pd.DataFrame(\n",
    "    data=[[lower_percentile90, positional_accuracy_distance, upper_percentile90, 'meters', '90p', version, county]], \n",
    "    columns=['lower_bound', 'calculated_metric', 'upper_bound', 'units', 'metric', 'version', 'county'], index = None)\n",
    "results_sum = pd.concat([results_sum, new_result])\n",
    "results_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cd60f8c-e9ef-47d3-926e-6a0cf0b0f2f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### % of matches below 50m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c68e04a-99d5-4872-94b9-5a042f7edee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "proportion_50m_matches = (matches_df['mnr_query_distance'] <= 50).mean()\n",
    "nice_num_50m = round(proportion_50m_matches * 100, 1)\n",
    "print(f'The calculated percentage of matches within 50 meters is {nice_num_50m}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "norm_data,_ = yeojohnson(matches_df['mnr_query_distance'].loc[matches_df['mnr_query_distance']<=50])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.displot(norm_data, kind = \"kde\",color = \"#e64e4e\", height=10, aspect=2,\n",
    "            linewidth = 5 )\n",
    "ax.fig.suptitle('Data distribution', size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6cfa1071-164b-4e50-8619-ee0428d37f88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[lower_50m_pa, upper_50m_pa] = percentile_bootstrap(matches_df['mnr_query_distance'] <= 50, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c70d2e57-4224-4325-be5a-5fa3b28638f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "    version = date\n",
    "else:\n",
    "    version = 'New_benchmark'\n",
    "\n",
    "new_result = pd.DataFrame(\n",
    "     data=[[lower_50m_pa, proportion_50m_matches, upper_50m_pa, '%', 'APA', version, county]], \n",
    "    columns=['lower_bound', 'calculated_metric', 'upper_bound', 'units', 'metric', 'version', 'county'])\n",
    "results_sum = pd.concat([results_sum, new_result])\n",
    "results_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(sample_gdf.sample_id) == set(similarity_df.sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "93109947-2b60-4c20-8de0-00185ac9aee8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Join Matches Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "213278e7-5d2e-4c15-98e2-7c2be95d2df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matches_df[['sample_id', 'mnr_query_distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f15eb313-d79f-4eae-91ca-c9d61f8539e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df = match_df.merge(matches_df[['sample_id', 'mnr_query_distance']], on = ['sample_id'], how = 'left')\n",
    "match_df['apa'] = match_df.mnr_query_distance.apply(lambda x: 1 if x < 52 else 0)\n",
    "# match_df.drop('mnr_query_distance', axis = 1, inplace = True)\n",
    "# raw2p.write_to_db(match_df, table_name = 'matches_table', schema = 'STAN_169')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed4463eb-044f-4d0b-90fb-cab2a645977b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if delta:\n",
    "  match_df = match_df.merge(source_delta_gdf[['feat_id', 'geometry']], on = ['feat_id'], how = 'inner')\n",
    "  del source_delta_gdf\n",
    "else:\n",
    "  match_df = match_df.merge(source_gdf[['feat_id', 'geometry']], on = ['feat_id'], how = 'inner')\n",
    "  del source_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.geometry = match_df.geometry.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5faeb8c-05a5-4e5a-a642-6e101f69a042",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Store Results @ psql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6265dc51-b07a-429a-bcde-cb53e2c22fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0241a156-5ec1-4f46-a5ce-80cf686a377b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3c0c9695-75e6-4ebb-9711-c140009b8555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# raw2p.write_to_db(results_sum, table_name = 'results', schema = 'STAN_169')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "277479b5-8b32-4aea-9a79-0d1b7e45a1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storing Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5bdfd68e-834d-4a1a-8713-b1101f192a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df['version'] = date\n",
    "match_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20d493a0-abed-4381-81ee-96c2ba7420b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from shapely import wkt \n",
    "# raw2p.write_to_db(match_df, table_name = 'matches_table', schema = 'STAN_169')\n",
    "# match_df.to_csv(os.path.join(data_path,state,county,'Apt_realignment_MSFT/matching_df.csv'))\n",
    "match_df['mnr_query_distance'] = match_df['mnr_query_distance'].apply(lambda x :float(x))\n",
    "match_df['geometry'] = match_df['geometry'].apply(wkt.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51e4876d-86db-4490-948b-16c672bcea6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(match_df[['feat_id','sample_id','apa','asf','geometry','mnr_query_distance']], crs=\"epsg:4326\", geometry='geometry')\n",
    "gdf.to_crs(\"epsg:4326\")\n",
    "gdf.to_file(os.path.join(data_path,state,county,'Apt_realignment_MSFT_OSM/matching_df.shp'), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "path = \"/mnt/c/Users/tandon/OneDrive - TomTom/Desktop/tomtom/Workspace/01_Rooftop_accuracy/APT-Realignment/data/Colorado/Apt_realignment_MSFT/updated_geometries_bfp-count_1_Parcels_08003/APT_realigned.pkl\"\n",
    "\n",
    "\n",
    "df = pd.read_pickle(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample_updated_APT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "final_process",
   "notebookOrigID": 2036890387749244,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pygeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "70ce54ad97a74852893fb3a2eed7fab00a48abaab9c8d77c9e0b6a3e4b45b116"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
